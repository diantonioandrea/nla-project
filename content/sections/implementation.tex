\subsection{Subspace Embedding} \label{sseq:embdedding}

The subspace embedding of choice is the sparse reduction map. Fix $s = 2 \left( d + 1 \right)$, which typically yields $\varepsilon \approx 1 / \sqrt{2}$, and construct $\Matrix{S} \in \RealMatrices{s}{n}$ such that each column has exactly $\lceil 2 \log \left( d + 1 \right) \rceil$ nonzero entries placed at uniformly random coordinates. The values of these entries are chosen uniformly at random to be either $1 / \sqrt{s}$ or $-1 / \sqrt{s}$.

The choice of this subspace embedding is motivated by the need for fast matrix-vector multiplications and low memory consumption.

\subsection{Basis Construction} \label{sseq:basis}

A possibility for the basis construction for the \textit{sGMRES} is to build an orthonormal, or partially orthonormal, basis $\Matrix{B}$ of the Krilov subspace $\mathcal{K}(\Matrix{A}; \Vector{r}_0)$.

The following algorithm illustrates the k-truncated Arnoldi process which, fixed $k \in \NaturalNumbers$ with $k \ll d$, produce a k-orthonormal basis $\Matrix{B}$ for $\mathcal{K}(\Matrix{A}; \Vector{r}_0)$ and the least-squares matrix $\Matrix{M} \coloneqq \Matrix{A} \Matrix{B}$.

\begin{algorithm}
\caption{k-Truncated Arnoldi Method} \label{alg:arnoldi}
\setstretch{1.15}
\begin{algorithmic}
\Require $\Matrix{A} \in \RealMatrices{n}{n}$, $\Vector{r}_0 \in \RealVectors{n}$, $d, k \in \NaturalNumbers$
\State $\Matrix{B}_1 \gets \Vector{r}_0 / \Norm{\Vector{r}_0}_2$
\State $\Matrix{M}_1 \gets \Matrix{A} \Matrix{B}_1$
\For{$j = 2, \dots, k$}
    \State $\Matrix{B}_j \gets \left( \mathbf{I}_n - \sum_{i=1}^{j-1} \Matrix{B}_i \Matrix{B}_i^{\intercal} \right) \Matrix{M}_{j-1}$
    \State $\Matrix{B}_j \gets \Matrix{B}_j / \Norm{\Matrix{B}_j}_2$
    \State $\Matrix{M}_j \gets \Matrix{A} \Matrix{B}_j$
\EndFor
\For{$j = k + 1, \dots, d$}
    \State $\Matrix{B}_j \gets \left( \mathbf{I}_n - \sum_{i=j-k}^{j-1} \Matrix{B}_i \Matrix{B}_i^{\intercal} \right) \Matrix{M}_{j-1}$
    \State $\Matrix{B}_j \gets \Matrix{B}_j / \Norm{\Matrix{B}_j}_2$
    \State $\Matrix{M}_j \gets \Matrix{A} \Matrix{B}_j$
\EndFor
\end{algorithmic}
\end{algorithm}

The k-Truncated Arnoldi method reduces the cost of basis construction from $\BigO{n d^2}$ to $\BigO{n k^2}$ while still producing accurate solutions, provided that the reduced matrix $\Matrix{A} \Matrix{B}$ is reasonably well-conditioned. The parameter $k$ is typically chosen to be a small number, such as $2$ or $4$.

\newpage
\subsection{Sketched Least-Squares Problem} \label{sseq:least_squares}

The sketched least-squares problem is solved by the means of a QR factorization\footnote{May require pivoting.} of the sketched matrix $\Matrix{S} \Matrix{A} \Matrix{B}$. Let $\Matrix{Q} \in \RealMatrices{s}{d}$ orthogonal and $\Matrix{R} \in \RealMatrices{d}{d}$ upper triangular such that:
\begin{align}
    \Matrix{S} \Matrix{A} \Matrix{B} = \Matrix{Q} \Matrix{R},
\end{align}
then a solution of \cref{eq:sketched_min_problem} is given by\footnote{$\Matrix{R}^{-1}$ is applied by back substitution.}:
\begin{align}
    \Vector{y} = \Matrix{R}^{-1} \Matrix{Q}^{\intercal} \Matrix{S} \Vector{r}_0.
\end{align}

This leads to the following form for the residual estimate:
\begin{align}
    \Norm{\Vector{r}}_2 = \Norm{ \left( \mathbf{I}_s - \Matrix{Q} \Matrix{Q}^{\intercal} \right) \Matrix{S} \Vector{r}_0 }_2.
\end{align}

These operations both require $\BigO{d^3}$ arithmetic given that $s = \BigO{d}$.

The approximate solution $\Vector{x} = \Vector{x}_0 + \Matrix{B} \Vector{y}$ is then explicitly formed at a cost of $\BigO{n d}$ operations.

\subsection{Full Algorithm} \label{sseq:algorithm}

The following algorithm summarizes the \textit{sGMRES} method as implemented for this report.

\begin{algorithm}
\caption{\textit{k-sGMRES}} \label{alg:sgmres}
\setstretch{1.15}
\begin{algorithmic}
\Require $\Matrix{A} \in \RealMatrices{n}{n}$, $\Vector{b} \in \RealVectors{n}$, $\Vector{x}_0 \in \RealVectors{n}$, $d, k \in \NaturalNumbers$
\Ensure $\Vector{x} \in \RealVectors{n}$ approximate solution to $\Matrix{A} \Vector{x} = \Vector{b}$
\State $\Vector{r}_0 \gets \Vector{b} - \Matrix{A} \Vector{x}_0$
\State Set $s = 2 (d + 1)$ and construct $\Matrix{S} \in \RealMatrices{s}{n}$ subspace embedding \Comment Refer to \cref{sseq:embdedding}.
\State Apply \cref{alg:arnoldi} to construct $\Matrix{B} \in \RealMatrices{n}{d}$ and $\Matrix{M} \coloneqq \Matrix{A} \Matrix{B}$ \Comment Refer to \cref{sseq:basis}.
\State Sketch reduced matrix $\Matrix{S} \Matrix{M}$ and construct its QR factorization $\Matrix{S} \Matrix{M} = \Matrix{Q} \Matrix{R}$
\State Solve the least-squares problem $\Vector{y} = \Matrix{R}^{-1} \Matrix{Q}^{\intercal} \Matrix{S} \Vector{r}_0$
\State Evaluate the residual estimate $\Norm{ \left( \mathbf{I}_s - \Matrix{Q} \Matrix{Q}^{\intercal} \right) \Matrix{S} \Vector{r}_0 }_2$
\State Construct the approximate solution $\Vector{x} = \Vector{x}_0 + \Matrix{B} \Vector{y}$
\end{algorithmic}
\end{algorithm}