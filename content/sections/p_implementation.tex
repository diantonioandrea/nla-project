\subsection{Subspace Embedding}

\begin{frame}
    \frametitle{Subspace Embedding}

    The subspace embedding of choice is the sparse reduction map. 
    
    Fix $s = 2 \left( d + 1 \right)$, which typically yields $\varepsilon \approx 1 / \sqrt{2}$.
    
    Each column has exactly $\lceil 2 \log \left( d + 1 \right) \rceil$ nonzero entries, placed at uniformly random coordinates. The values of these entries are chosen uniformly at random to be either $1 / \sqrt{s}$ or $-1 / \sqrt{s}$.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementation}

    The following snippet illustrates the construction of the embedding as a CSC matrix. Some lines are omitted.

    The triplet \lstinline{Nv0, Nv1, Rv0} characterizes the matrix.

\begin{lstlisting}[style=cpp]
const natural_t N2 = 2 * (N0 + 1);
const natural_t N3 = 
    static_cast<natural_t>(std::ceil(2.0 * std::log(N0 + 1.0)));

const real_t R0 =
    1.0 / std::sqrt(static_cast<real_t>(N2)), R1 = -R0;

for(natural_t N4 = 0; N4 < N1; ++N4)
    for(natural_t N5 = 0; N5 < N3; ++N5) {
        Nv1[N4 * N3 + N5] = std::rand() % N2;
        Rv0[N4 * N3 + N5] = (std::rand() % 2) ? R0 : R1;
    }
\end{lstlisting}

\end{frame}

\subsection{Basis Construction}

\begin{frame}
    \frametitle{Basis Construction}

    A possible approach for constructing the basis for the \textit{sGMRES} algorithm is to build an orthonormal, or partially orthonormal, basis $\Matrix{B}$ for the Krylov subspace $\mathcal{K}(\Matrix{A}; \Vector{r}_0)$.

    The k-Truncated Arnoldi method reduces the cost of basis construction from $\mathcal{O}(n d^2)$ to $\mathcal{O}(n k^2)$ while still producing accurate solutions, provided that the reduced matrix $\Matrix{A} \Matrix{B}$ is reasonably well-conditioned. The parameter $k$ is typically chosen to be a small number, such as $2$ or $4$.
\end{frame}

\begin{frame}[fragile]
    \frametitle{Algorithm}

    The following algorithm illustrates the k-truncated Arnoldi process.

    \begin{algorithm}[H]
    \caption{k-Truncated Arnoldi Method} \label{alg:arnoldi}
    \setstretch{1.15}
    \begin{algorithmic}
    \Require $\Matrix{A} \in \RealMatrices{n}{n}$, $\Vector{r}_0 \in \RealVectors{n}$, $d, k \in \NaturalNumbers$
    \State $\Matrix{B}_1 \gets \Vector{r}_0 / \Norm{\Vector{r}_0}_2$
    \State $\Matrix{M}_1 \gets \Matrix{A} \Matrix{B}_1$
    \For{$j = 2, \dots, k$}
        \State $\Matrix{B}_j \gets \left( \mathbf{I}_n - \sum_{i=1}^{j-1} \Matrix{B}_i \Matrix{B}_i^{\intercal} \right) \Matrix{M}_{j-1}$
        \State $\Matrix{B}_j \gets \Matrix{B}_j / \Norm{\Matrix{B}_j}_2$
        \State $\Matrix{M}_j \gets \Matrix{A} \Matrix{B}_j$
    \EndFor
    \For{$j = k + 1, \dots, d$}
        \State $\Matrix{B}_j \gets \left( \mathbf{I}_n - \sum_{i=j-k}^{j-1} \Matrix{B}_i \Matrix{B}_i^{\intercal} \right) \Matrix{M}_{j-1}$
        \State $\Matrix{B}_j \gets \Matrix{B}_j / \Norm{\Matrix{B}_j}_2$
        \State $\Matrix{M}_j \gets \Matrix{A} \Matrix{B}_j$
    \EndFor
    \end{algorithmic}
    \end{algorithm}

\end{frame}

\begin{frame}[fragile]
    \frametitle{Implementation}

    The following snippet illustrates the second part of \cref{alg:arnoldi}.

\begin{lstlisting}[style=cpp]
for(natural_t N4 = N2 + 1; N4 < N1; ++N4) {

    // Copy.
    Cp_RvtRvN_0(Rm1 + N4 * N0, Rm2 + (N4 - 1) * N0, N0);

    // (Re-)orthogonalization.
    for(natural_t N5 = 0; N5 < 2; ++N5)
        for(natural_t N6 = N4 - N2; N6 < N4; ++N6)
            Prj_RvtRvRvN_0(
                Rm1 + N4 * N0, Rm1 + N6 * N0, Rm1 + N4 * N0, N0);

    // Normalization.
    Nrz_RvtN_0(Rm1 + N4 * N0, N0);

    // LS matrix.
    Mlc_RvtNNvNvRvRv_0(
        Rm2 + N4 * N0, N0, Nv0, Nv1, Rv0, Rm1 + N4 * N0);
}
\end{lstlisting}

\end{frame}

\begin{frame}
    \frametitle{Memory Considerations}

    Since storage and memory are limited for large-scale problems (i.e., $n \gg \num{1e6}$), the Arnoldi process is implemented in a streaming fashion:
    \begin{itemize}
        \item The basis $\Matrix{B}$ is constructed and discarded twice: once during the construction of $\Matrix{S} \Matrix{M}$ and again during the evaluation of $\Vector{x}$.
        \item Only $k n$ values need to be stored at any given time, as opposed to $d n$.
    \end{itemize}
\end{frame}

\subsection{Sketched Least-Squares Problem}

\begin{frame}
    \frametitle{Sketched Least-Squares Problem}

    The sketched least-squares problem is solved using a QR factorization of the sketched matrix $\Matrix{S} \Matrix{A} \Matrix{B}$.

    Let $\Matrix{Q} \in \RealMatrices{s}{d}$ be orthogonal and $\Matrix{R} \in \RealMatrices{d}{d}$ upper triangular such that:
    \begin{align}
        \Matrix{S} \Matrix{A} \Matrix{B} = \Matrix{Q} \Matrix{R},
    \end{align}
    then a solution of the sketched minimization problem is given by:
    \begin{align}
        \Vector{y} = \Matrix{R}^{-1} \Matrix{Q}^{\intercal} \Matrix{S} \Vector{r}_0.
    \end{align}
\end{frame}

\begin{frame}
    \frametitle{Residual Estimate}

    The residual estimate for the sketched least-squares problem is given by:
    \begin{align}
        \Norm{\Vector{r}}_2 = \Norm{ \left( \mathbf{I}_s - \Matrix{Q} \Matrix{Q}^{\intercal} \right) \Matrix{S} \Vector{r}_0 }_2.
    \end{align}
    
    These operations require $\BigO{d^3}$ arithmetic, assuming $s = \BigO{d}$.
\end{frame}

\begin{frame}
    \frametitle{Approximate Solution}

    The approximate solution is explicitly formed as:
    \begin{align}
        \Vector{x} = \Vector{x}_0 + \Matrix{B} \Vector{y},
    \end{align}
    at a cost of $\BigO{n d}$ operations.
\end{frame}

\subsection{Full Algorithm}

\begin{frame}[fragile]
    \frametitle{Algorithm: \textit{k-sGMRES}}

    The following algorithm summarizes the \textit{sGMRES} method.

    \begin{algorithm}[H]
    \caption{\textit{k-sGMRES}} \label{alg:sgmres}
    \setstretch{1.15}
    \begin{algorithmic}
    \Require $\Matrix{A} \in \RealMatrices{n}{n}$, $\Vector{b} \in \RealVectors{n}$, $\Vector{x}_0 \in \RealVectors{n}$, $d, k \in \NaturalNumbers$
    \Ensure $\Vector{x} \in \RealVectors{n}$ approximate solution to $\Matrix{A} \Vector{x} = \Vector{b}$
    \State $\Vector{r}_0 \gets \Vector{b} - \Matrix{A} \Vector{x}_0$
    \State Set $s = 2 (d + 1)$ and construct $\Matrix{S} \in \RealMatrices{s}{n}$ subspace embedding
    \State Apply \cref{alg:arnoldi} to construct $\Matrix{B} \in \RealMatrices{n}{d}$ and $\Matrix{M} \coloneqq \Matrix{A} \Matrix{B}$
    \State Sketch reduced matrix $\Matrix{S} \Matrix{M}$ and construct its QR factorization $\Matrix{S} \Matrix{M} = \Matrix{Q} \Matrix{R}$
    \State Solve the least-squares problem $\Vector{y} = \Matrix{R}^{-1} \Matrix{Q}^{\intercal} \Matrix{S} \Vector{r}_0$
    \State Evaluate the residual estimate $\Norm{ \left( \mathbf{I}_s - \Matrix{Q} \Matrix{Q}^{\intercal} \right) \Matrix{S} \Vector{r}_0 }_2$
    \State Construct the approximate solution $\Vector{x} = \Vector{x}_0 + \Matrix{B} \Vector{y}$
    \end{algorithmic}
    \end{algorithm}

\end{frame}