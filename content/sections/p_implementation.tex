\subsection{Subspace Embedding}

\begin{frame}{Sparse Reduction Map}
    \begin{itemize}
        \item Parameter selection:
        \begin{itemize}
            \item Fix $s = 2 \left( d + 1 \right)$, typically leading to $\varepsilon \approx \frac{1}{\sqrt{2}}$.
        \end{itemize}
        \item Construction details:
        \begin{itemize}
            \item Each column has $\lceil 2 \log \left( d + 1 \right) \rceil$ nonzero entries.
            \item Nonzero entries are placed at uniformly random coordinates.
            \item Entry values are chosen uniformly at random as either $1 / \sqrt{s}$ or $-1 / \sqrt{s}$.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile] % Subspace embedding.
    \frametitle{Implementation}

\begin{lstlisting}[style=cpp]
#pragma omp parallel for
for(natural_t N4 = 0; N4 < N1 + 1; ++N4)
    Nv0[N4] = N4 * N3;

for(natural_t N4 = 0; N4 < N1; ++N4)
    for(natural_t N5 = 0; N5 < N3; ++N5) {
        Nv1[N4 * N3 + N5] = std::rand() % N2;
        Rv0[N4 * N3 + N5] = (std::rand() % 2) ? R0 : R1;
    }
\end{lstlisting}

\end{frame}

\subsection{Basis Construction}

\begin{frame}{Basis Construction}
    \begin{itemize}
        \item \textbf{Basis for the \textit{sGMRES} Algorithm}
        \begin{itemize}
            \item Build an orthonormal or partially orthonormal basis $\Matrix{B}$.
            \item The basis spans the Krylov subspace $\mathcal{K}(\Matrix{A}; \Vector{r}_0)$.
        \end{itemize}
    
        \item \textbf{k-Truncated Arnoldi Method}
        \begin{itemize}
            \item Reduces the computational cost of basis construction from $\mathcal{O}(n d^2)$ to $\mathcal{O}(n k^2)$.
            \item Ensures accurate solutions if $\Matrix{A} \Matrix{B}$ is well-conditioned.
            \item The parameter $k$ is typically small, such as $k = 2$ or $k = 4$.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Algorithm}

    The following algorithm illustrates the k-truncated Arnoldi process.

    \begin{algorithm}[H]
    \caption{k-Truncated Arnoldi Method} \label{alg:arnoldi}
    \setstretch{1.15}
    \begin{algorithmic}
    \Require $\Matrix{A} \in \RealMatrices{n}{n}$, $\Vector{r}_0 \in \RealVectors{n}$, $d, k \in \NaturalNumbers$
    \State $\Matrix{B}_1 \gets \Vector{r}_0 / \Norm{\Vector{r}_0}_2$
    \State $\Matrix{M}_1 \gets \Matrix{A} \Matrix{B}_1$
    \For{$j = 2, \dots, k$}
        \State $\Matrix{B}_j \gets \left( \mathbf{I}_n - \sum_{i=1}^{j-1} \Matrix{B}_i \Matrix{B}_i^{\intercal} \right) \Matrix{M}_{j-1}$
        \State $\Matrix{B}_j \gets \Matrix{B}_j / \Norm{\Matrix{B}_j}_2$
        \State $\Matrix{M}_j \gets \Matrix{A} \Matrix{B}_j$
    \EndFor
    \For{$j = k + 1, \dots, d$}
        \State $\Matrix{B}_j \gets \left( \mathbf{I}_n - \sum_{i=j-k}^{j-1} \Matrix{B}_i \Matrix{B}_i^{\intercal} \right) \Matrix{M}_{j-1}$
        \State $\Matrix{B}_j \gets \Matrix{B}_j / \Norm{\Matrix{B}_j}_2$
        \State $\Matrix{M}_j \gets \Matrix{A} \Matrix{B}_j$
    \EndFor
    \end{algorithmic}
    \end{algorithm}

\end{frame}

\begin{frame}{Memory Considerations}
    \begin{itemize}
        \item Large-scale problems ($n \gg \num{1e6}$) impose strict storage and memory limitations.
        \item To address this, the Arnoldi process is implemented in a \textbf{streaming fashion}:
        \begin{itemize}
            \item The basis $\Matrix{B}$ is constructed and discarded twice:
            \begin{itemize}
                \item Once during the construction of $\Matrix{S} \Matrix{M}$.
                \item Once during the evaluation of $\Vector{x}$.
            \end{itemize}
            \item At any time, only $k n$ values are stored.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile] % Arnoldi, first part.
    \frametitle{Implementation}

\begin{lstlisting}[style=cpp]
for(natural_t N4 = 1; N4 <= N2; ++N4) {

    Mlc_RvtNNvNvRvRv_0(
        Rm1 + N4 * N0, N0, Nv0, Nv1, Rv0, Rm1 + (N4 - 1) * N0);

    Mlc_RvtNNvNvRvRv_0(
        Rm3 + (N4 - 1) * N3, N0, Nv2, Nv3, Rv2, Rm1 + N4 * N0);

    for(natural_t N5 = 0; N5 < 2; ++N5)
        for(natural_t N6 = 0; N6 < N4; ++N6)
            Prj_RvtRvRvN_0(
                Rm1 + N4 * N0, Rm1 + N6 * N0, Rm1 + N4 * N0, N0);

    Nrz_RvtN_0(Rm1 + N4 * N0, N0);
}
\end{lstlisting}

\end{frame}

\begin{frame}[fragile] % Arnoldi, second part. 1/3.
    \frametitle{Implementation}

\begin{lstlisting}[style=cpp]
for(natural_t N4 = N2 + 1; N4 < N1; ++N4) {

    const natural_t N5 = N4 % (N2 + 1);
    const natural_t N6 = (N4 - 1) % (N2 + 1);

    #pragma omp parallel for
    for(natural_t N7 = 0; N7 < N0; ++N7)
        Rm1[N5 * N0 + N7] = 0.0;

    Mlc_RvtNNvNvRvRv_0(
        Rm1 + N5 * N0, N0, Nv0, Nv1, Rv0, Rm1 + N6 * N0);

    Mlc_RvtNNvNvRvRv_0(
        Rm3 + (N4 - 1) * N3, N0, Nv2, Nv3, Rv2, Rm1 + N5 * N0);

    [...]
\end{lstlisting}

\end{frame}

\begin{frame}[fragile] % Arnoldi, second part. 2/3.
    \frametitle{Implementation}

\begin{lstlisting}[style=cpp]
    [...]

    for(natural_t N7 = 0; N7 < 2; ++N7)
        for(natural_t N8 = 0; N8 <= N2; ++N8) {
            if(N5 == N8)
                continue;

            Prj_RvtRvRvN_0(
                Rm1 + N5 * N0, Rm1 + N8 * N0, Rm1 + N5 * N0, N0);
        }

    Nrz_RvtN_0(Rm1 + N5 * N0, N0);
}

[...]
\end{lstlisting}

\end{frame}

\begin{frame}[fragile] % Arnoldi, second part. 3/3.
    \frametitle{Implementation}

\begin{lstlisting}[style=cpp]
[...]

#pragma omp parallel for
for(natural_t N7 = 0; N7 < N0; ++N7)
    Rm1[(N1 % (N2 + 1)) * N0 + N7] = 0.0;

Mlc_RvtNNvNvRvRv_0(
    Rm1 + (N1 % (N2 + 1)) * N0,
        N0, Nv0, Nv1, Rv0, Rm1 + ((N1 - 1) % (N2 + 1)) * N0);

Mlc_RvtNNvNvRvRv_0(
    Rm3 + (N1 - 1) * N3,
        N0, Nv2, Nv3, Rv2, Rm1 + (N1 % (N2 + 1)) * N0);
\end{lstlisting}

\end{frame}

\subsection{Sketched Least-Squares Problem}

\begin{frame}{Sketched Least-Squares Problem}
    \begin{itemize}
        \item \textbf{Least-Squares Problem}
        \begin{itemize}
            \item Solve the sketched least-squares problem using a QR factorization of $\Matrix{S} \Matrix{A} \Matrix{B}$:
            \begin{align}
                \Matrix{S} \Matrix{A} \Matrix{B} = \Matrix{Q} \Matrix{R},
            \end{align}
            where:
            \begin{itemize}
                \item $\Matrix{Q} \in \RealMatrices{s}{d}$ is orthogonal.
                \item $\Matrix{R} \in \RealMatrices{d}{d}$ is upper triangular.
            \end{itemize}
            \item The solution is:
            \begin{align}
                \Vector{y} = \Matrix{R}^{-1} \Matrix{Q}^{\intercal} \Matrix{S} \Vector{r}_0.
            \end{align}
            \item Computational cost: $\BigO{d^3}$, assuming $s = \BigO{d}$.
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile] % Sketched LS.
    \frametitle{Implementation}

\begin{lstlisting}[style=cpp]
Mqt_RvtRmNN_0(Rv5, Rm4, N3, N1, N1);

for(natural_t N4 = N1; N4 > 0; --N4) {
    real_t R2 = 0.0;

    for(natural_t N5 = N4; N5 < N1; ++N5)
        R2 += Rm3[N5 * N3 + N4 - 1] * Rv6[N5];

    Rv6[N4 - 1] = (Rv5[N4 - 1] - R2) / Rm3[(N4 - 1) * (N3 + 1)];
}

for(natural_t N4 = 0; N4 < N1; ++N4)
    Rv7[Nv4[N4]] = Rv6[N4];
\end{lstlisting}

\end{frame}

\begin{frame}{Approximate Solution and Residual Estimate}
    \begin{itemize}
        \item \textbf{Approximate Solution}
        \begin{itemize}
            \item The solution is explicitly formed as:
            \begin{align}
                \Vector{x} = \Vector{x}_0 + \Matrix{B} \Vector{y}.
            \end{align}
            \item Computational cost: $\BigO{n d}$.
        \end{itemize}
        \vspace{0.5cm}
        \item \textbf{Residual Estimate} 
        \begin{itemize}
            \item The residual norm is given by:
            \begin{align}
                \Norm{\Vector{r}}_2 = 
                \Norm{ \left( \mathbf{I}_s - \Matrix{Q} \Matrix{Q}^{\intercal} \right) \Matrix{S} \Vector{r}_0 }_2.
            \end{align}
        \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}[fragile] % Solution. 1/2.
    \frametitle{Implementation}

\begin{lstlisting}[style=cpp]
Cp_RvtRvN_0(Rm1, Rv4, N0);
Nrz_RvtN_0(Rm1, N0);

#pragma omp parallel for
for(natural_t N5 = 0; N5 < N0; ++N5)
    Rvt0[N5] += Rm1[N5] * Rv7[0];

for(natural_t N4 = 1; N4 <= N2; ++N4) {

    #pragma omp parallel for
    for(natural_t N5 = 0; N5 < N0; ++N5)
        Rm1[N4 * N0 + N5] = 0.0;

    Mlc_RvtNNvNvRvRv_0(
        Rm1 + N4 * N0, N0, Nv0, Nv1, Rv0, Rm1 + (N4 - 1) * N0);

    [...]
\end{lstlisting}

\end{frame}

\begin{frame}[fragile] % Solution. 2/2.
    \frametitle{Implementation}

\begin{lstlisting}[style=cpp]
    [...]

    for(natural_t N5 = 0; N5 < 2; ++N5)
        for(natural_t N6 = 0; N6 < N4; ++N6)
            Prj_RvtRvRvN_0(
                Rm1 + N4 * N0, Rm1 + N6 * N0, Rm1 + N4 * N0, N0);

    Nrz_RvtN_0(Rm1 + N4 * N0, N0);

    const real_t R3 = Rv7[N4];

    #pragma omp parallel for
    for(natural_t N5 = 0; N5 < N0; ++N5)
        Rvt0[N5] += Rm1[N4 * N0 + N5] * R3;
}
\end{lstlisting}

\end{frame}

\subsection{Full Algorithm}

\begin{frame}[fragile]
    \frametitle{\textit{k-sGMRES}}

    The following algorithm summarizes the \textit{sGMRES} method.

    \begin{algorithm}[H]
    \caption{\textit{k-sGMRES}} \label{alg:sgmres}
    \setstretch{1.15}
    \begin{algorithmic}
    \Require $\Matrix{A} \in \RealMatrices{n}{n}$, $\Vector{b} \in \RealVectors{n}$, $\Vector{x}_0 \in \RealVectors{n}$, $d, k \in \NaturalNumbers$
    \Ensure $\Vector{x} \in \RealVectors{n}$ approximate solution to $\Matrix{A} \Vector{x} = \Vector{b}$
    \State $\Vector{r}_0 \gets \Vector{b} - \Matrix{A} \Vector{x}_0$
    \State Set $s = 2 (d + 1)$ and construct $\Matrix{S} \in \RealMatrices{s}{n}$ subspace embedding
    \State Apply \cref{alg:arnoldi} to construct $\Matrix{B} \in \RealMatrices{n}{d}$ and $\Matrix{M} \coloneqq \Matrix{A} \Matrix{B}$
    \State Sketch reduced matrix $\Matrix{S} \Matrix{M}$ and construct its QR factorization $\Matrix{S} \Matrix{M} = \Matrix{Q} \Matrix{R}$
    \State Solve the least-squares problem $\Vector{y} = \Matrix{R}^{-1} \Matrix{Q}^{\intercal} \Matrix{S} \Vector{r}_0$
    \State Evaluate the residual estimate $\Norm{ \left( \mathbf{I}_s - \Matrix{Q} \Matrix{Q}^{\intercal} \right) \Matrix{S} \Vector{r}_0 }_2$
    \State Construct the approximate solution $\Vector{x} = \Vector{x}_0 + \Matrix{B} \Vector{y}$
    \end{algorithmic}
    \end{algorithm}

\end{frame}
